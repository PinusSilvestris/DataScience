---
title: "Kalman Filters"
author: "Krzysztof Suwada"
date: "29 kwietnia 2016"
output: 
  html_document: 
    theme: cerulean
---
```{r}
# http://www.magesblog.com/2015/01/kalman-filter-example-visualised-with-r.html
# http://www.quant-econ.net/jl/kalman.html
# http://www.rmki.kfki.hu/~banmi/elte/Bishop%20-%20Pattern%20Recognition%20and%20Machine%20Learning.pdf, page 93
# https://en.wikipedia.org/wiki/Woodbury_matrix_identity
```
```{r echo=FALSE, warning=FALSE, message=FALSE}
source(file = "KalmanScript.R")
```

### Kalman filter###
This document is a intorduction to State Space Models (Unobserved Components Model). We focus on Kalman Filters.

Kalman Filter we define as

$$
\begin{aligned}
X_{t+1} &=  A_t X_t + W_{t+1}, & W_t \sim N(0,Q),&\quad \text{Time update}\\
Y_t &= G_t X_t + V_t, & V_t \sim N(0,R),&\quad \text{Measurement update}
\end{aligned}
$$

Matrices $A_t, G_t, R, Q$ are assumed as known. Please note that for the calculation we have random variables denoted with capital letters $X_t, Y_t$ and values denoted as $x_t, y_t$.

###The algorithm###
Input: Initial distribution of $X_1 \sim N(x_0,P_0)$, matrices $A_t, G_t, R, Q$, data $y_1, \ldots, y_T$.

$T$ is the length of the data - time window.

With Kalman approach we can have three things

* filtering - when we use information prior to the time $t, t-1, t-2,\ldots,0$.
* smoothing - when we use all the information for the time window
* prediction - when we want to predict the future values
    
Please note that we know not the exact value but a probability distribution.
Lets set the values as:

$$x_0 = (0.5, -0.5)$$
$$P_0 = \left[\begin{array}{cc} 
1 & 0.5\\ 
0.5 & 0.5 
\end{array} 
\right]$$

```{r echo=FALSE}
print(g1)
```

Given the prior distribution of $X_1$ we want to calculate the condidional distribution of $Y_1|X_1$ using measurement update equation. Given that and the distribution of $V_t \sim N(0,R)$, we have that $Y_1 | X_1 = x_0 \sim N(G x_0, R)$

Given a marginal Gaussian distribution of $X$ and a conditional Gaussian distribution of $Y|X$ the marginal distribution of $Y$ and conditional $X|Y$ are given by

$$\begin{aligned}
X&    \sim &  N(& m, & M)  \\ 
Y|X&  \sim &  N(&Ax+b,& N) \\
Y &   \sim &  N(&Am+b,& N+AM^{-1}A^T) \\
X|Y&  \sim &  N( &\Sigma [A^T N^{-1}(y-b)+M^{-1}m],& \Sigma)\\
&&&\Sigma = \left(M^{-1}+A^TN^{-1}A \right)^{-1}
\end{aligned}$$

```{r echo=FALSE}
print(g2)
```

Now we want to calculate $X_f = X_1 | Y_1 \sim N(x_f, P_f)$. Given the fact that all distributions are normal we have the following formula

$$\begin{aligned}
x_f &=& (P_0^{-1} + G^T R^{-1} G)^{-1} (P_0^{-1} x_0 + G^T R^{-1} y_1)  \\ 
P_f &=& (P_0^{-1} + G^T R^{-1} G)^{-1}
\end{aligned}$$

Now we apply Woodbury matrix identity 
$$\left( A + UCV\right)^{-1} = A^{-1}-A^{-1}U\left(C^{-1}+VA^{-1}U\right)^{-1}VA^{-1}$$
 
Using the formula for matrix inversion mentioned above we get the following.
$$
\begin{aligned}
x_f &=& x_0+P_0 G^T (G P_0 G^T +R)^{-1} (y_1 - G x_0)  \\ 
P_f &=& P_0 - P_0 G^T (G P_0 G^T + R)^{-1} G P_0
\end{aligned}
$$

```{r echo=FALSE}
print(g3)
```

The last step is to calculate the unconditioned distribution of $X_1$. 
$$X_{t+1} = A X_t + W_{t+1} \mbox{, where } W_t \sim N(0, Q)$$

It is known that a linear transformation of gaussian vector remain gaussian. We only have to calculate the mean and the variance.

$\begin{align} 
\mathbb{E} [A X_f + W] &= A \mathbb{E} [X_f] + \mathbb{E} [W]\\ 
&= A x_f\\ 
&= A x_0 + A P_0 G^T (G P_0 G^T + R)^{-1}(y - G x_0) 
\end{align}$

$\begin{align} 
\operatorname{Var} [A X_f + W] &= A \operatorname{Var}[X_f] A^T + Q\\ 
&= A P_f A^T + Q\\ 
&= A P_0 A^T - A P_0 G^T (G P_0 G^T + R)^{-1} G P_0 A^T + Q 
\end{align}$

The matrix $K_\Sigma = A P_0 G^T (G P_0 G^T + R)^{-1}$ is known as Kalman gain.

```{r echo=FALSE}
print(g4)
```


So the general formulas to update the expected value and covariance matrix are as follows

$\begin{align} 
x_{t+1} &=& A x_t + K_\Sigma(y_t-G x_t) \\
P_{t+1} &=& A P_t A^T - K_\Sigma G P_t A^T+Q \\
K_\Sigma &=& A P_t G^T (G P_t G^T + R)^{-1}
\end{align}$

### Backward pass###

Now we are going to use ,,all'' the information. Note that

$(X_t, X_{t+1}) \sim N( 
\left[\begin{array}{c} x_t\\ 
x_{t+1}
\end{array}\right],
\left[\begin{array}{cc} P_t& P_t A^T\\ 
A P_t& P_{t+1}
\end{array}\right])$
 
Recall that if
$(Z_1, Z_2) \sim N( 
\left[\begin{array}{c} \mu_1\\ 
\mu_2
\end{array}\right],
\left[\begin{array}{cc} \Sigma_{11}& \Sigma_{12}\\ 
\Sigma_{21}& \Sigma_{22}
\end{array}\right])$

then $Z_1 | Z_2=z_2 \sim N(\mu_1+\Sigma_{12}\Sigma_{22}^{-1}(z_2-\mu_2),\Sigma_{11}-\Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21})$

Now lets compute $X_t | X_{t+1}=x_{t+1} \sim N(\mu_1+\Sigma_{12}\Sigma_{22}^{-1}(z_2-\mu_2),\Sigma_{11}-\Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21})$


