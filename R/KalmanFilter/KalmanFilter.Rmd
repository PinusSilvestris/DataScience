---
title: "KalmanFilters"
author: "Krzysztof Suwada"
date: "29 kwietnia 2016"
output: 
  html_document: 
    theme: cerulean
---
```{r}
# http://www.magesblog.com/2015/01/kalman-filter-example-visualised-with-r.html
# http://www.quant-econ.net/jl/kalman.html
```
```{r echo=FALSE, warning=FALSE, message=FALSE}
source(file = "KalmanScript.R")
```

This document is a intorduction to State Space Models (Unobserved Components Model). We focus on Kalman Filters.

###Kalman Filter we define as###

$$
\begin{aligned}
X_{t+1} &=  A_t X_t + W_{t+1}, & W_t \sim N(0,Q),&\quad \text{Time update}\\
Y_t &= G_t X_t + V_t, & V_t \sim N(0,R),&\quad \text{Measurement update}
\end{aligned}
$$

Matrices $A_t, G_t, R, Q$ are assumed as known. Please note that for the calculation we have random variables denoted with capital letters $X_t, Y_t$ and values denoted as $x_t, y_t$.

###The algorithm###
Input: Initial distribution of $X_1 \sim N(x_0,P_0)$, matrices $A_t, G_t, R, Q$, data $y_1, \ldots, y_T$.

$T$ is the length of the data - time window.

With Kalman approach we can have three things

* filtering - when we use information prior to the time $t, t-1, t-2,\ldots,0$.
* smoothing - when we use all the information for the time window
* prediction - when we want to predict the future values
    
Please note that we know not the exact value but a probability distribution.
Lets set the values as:

$$x_0 = (0.5, -0.5)$$
$$P_0 = \left[\begin{array}{cc} 
1 & 0.5\\ 
0.5 & 0.5 
\end{array} 
\right]$$

```{r echo=FALSE}
# print(g1)
```

Given the prior distribution of $X_1$ we want to calculate the condidional distribution of $Y_1|X_1$ using measurement update equation. Given that and the distribution of $V_t \sim N(0,R)$, we have that $Y_1 | X_1 = x_0 \sim N(G x_0, R)$

Given a marginal Gaussian distribution of $X$ and a conditional Gaussian distribution of $Y|X$ the marginal distribution of $Y$ and conditional $X|Y$ are given by

$$\begin{aligned}
X&    \sim &  N(& m, & M)  \\ 
Y|X&  \sim &  N(&Ax+b,& N) \\
Y &   \sim &  N(&Am+b,& N+AM^{-1}A^T) \\
X|Y&  \sim &  N( &\Sigma [A^T N^{-1}(y-b)+M^{-1}m],& \Sigma)\\
&&&\Sigma = \left(M^{-1}+A^TN^{-1}A \right)^{-1}
\end{aligned}$$

```{r echo=FALSE}
# print(g2)
```

Now we want to calculate $X_f = X_1 | Y_1 \sim N(x_f, P_f)$. Given the fact that all distributions are normal we have the following formula

$$\begin{aligned}
x_f = (P_0^{-1} + G^T R^{-1} G)^{-1} (P_0^{-1} x_0 + G^T R^{-1} y_1)  \\ 
P_f = (P_0^{-1} + G^T R^{-1} G)^{-1}
\end{aligned}$$

Now we apply Woodbury matrix identity 
$$\left( A + UCV\right)^{-1} = A^{-1}-A^{-1}U\left(C^{-1}+VA^{-1}U\right)^{-1}VA^{-1}$$
 
Using the formula for matrix inversion mentioned above we get the following.
$$
\begin{aligned}
x_f = x_0+P_0 G^T (G P_0 G^T +R)^{-1} (y_1 - G x_0)  \\ 
P_f = P_0 - P_0 G^T (G P_0 G^T + R)^{-1} G P_0
\end{aligned}
$$

```{r echo=FALSE}
# print(g3)
```

The last step is to calculate the unconditioned distribution of $X_1$. 
$$X_{t+1} = A X_t + W_{t+1} \mbox{, where } W_t \sim N(0, Q)$$

It is known that a linear transformation of gaussian vector remain gaussian. We only have to calculate the mean and the variance.

$\begin{align} 
\mathbb{E} [A X_f + W] &= A \mathbb{E} [X_f] + \mathbb{E} [W]\\ 
&= A \hat{x}_f\\ 
&= A \hat{x_0} + A P_0 G^T (G P_0 G^T + R)^{-1}(y - G \hat x_0) 
\end{align}$

$\begin{align} 
\operatorname{Var} [A X_f + W] &= A \operatorname{Var}[x_f] A^T + Q\\ 
&= A P_f A^T + Q\\ 
&= A P_0 A^T - A P_0 G^T (G P_0 G' + R)^{-1} G P_0 A^T + Q 
\end{align}$

The matrix $K_\Sigma = A P_0 G^T (G P_0 G^T + R)^{-1}$ is known as Kalman gain matrix.